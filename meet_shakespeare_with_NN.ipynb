{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMGW9468EEnU"
      },
      "source": [
        "# Advanced ML Part II // Lecture 07 Scratch // includes RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cnVlBAz3EEnY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as spio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNCkAe1TEpyP",
        "outputId": "2208a2a5-8821-4b2d-b769-1cb1b955a31b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wZER6o-JEEnb"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    e_ = np.exp(x - np.max(x))\n",
        "    return e_ / e_.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc3OtJCQEEnc"
      },
      "source": [
        "### Some basic loading and inspection of Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SiW6cF3HEEnc"
      },
      "outputs": [],
      "source": [
        "# load and inspect data\n",
        "shakes = open('shakespeare_input.txt')\n",
        "l = 0\n",
        "for line in shakes:\n",
        "    if l < 20: \n",
        "        print(line, end=\"\")\n",
        "        l += 1\n",
        "    else:\n",
        "        shakes.close()\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huOc5XiJEEnd"
      },
      "outputs": [],
      "source": [
        "# load and format data\n",
        "all_text = open('shakespeare_input.txt').read() \n",
        "#all_text = 'the quick brown fox jumped over the lazy dog '*10000\n",
        "charset = list(set(all_text))\n",
        "n_in = len(charset)\n",
        "n_train = len(all_text)\n",
        "print('The {} inputs are: \\n {}'.format(n_in,charset))\n",
        "# useful lookups\n",
        "ch2ind = {x:y for x,y in zip(charset,range(n_in))}\n",
        "ind2ch = {x:y for y,x in ch2ind.items()}\n",
        "# useful helper functions\n",
        "def ch2onehot(x):\n",
        "    return (np.arange(n_in) == np.array([ch2ind[xi] for xi in x]).reshape([-1,1])).astype(int)\n",
        "def ind2onehot(x):\n",
        "    return (np.arange(n_in) == np.array(x).reshape([-1,1])).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iMSWOkrEEne"
      },
      "source": [
        "### Construct simple input output Network (cf unigram model, aka no hidden state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AIHePwDEEne"
      },
      "outputs": [],
      "source": [
        "class Network0:\n",
        "    \n",
        "    def __init__(self, session, n_in , n_out):\n",
        "        self.session = session\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.n_hidden = 60\n",
        "        # data placeholders\n",
        "        self.x = tf.placeholder(tf.float32, [None, n_in], name='x')\n",
        "        self.y = tf.placeholder(tf.float32, [None, n_out], name='y')\n",
        "        self.x_in = tf.reshape(self.x, [-1,self.n_in])\n",
        "        # 2 layer network\n",
        "        self.W_fc1 = tf.get_variable('W_fc1', shape=[self.n_in,self.n_hidden])\n",
        "        self.b_fc1 = tf.get_variable('b_fc1', shape=[self.n_hidden])\n",
        "        self.h_fc1 = tf.nn.relu(tf.add(tf.matmul(self.x_in, self.W_fc1), self.b_fc1, name='layer1'))\n",
        "        self.W_fc2 = tf.get_variable('W_fc2', shape=[self.n_hidden,self.n_out])\n",
        "        self.b_fc2 = tf.get_variable('b_fc2', shape=[self.n_out])\n",
        "        self.logits = tf.add(tf.matmul(self.h_fc1, self.W_fc2), self.b_fc2, name='layer2')\n",
        "        self.ypred = tf.nn.softmax(self.logits)\n",
        "        # loss, train_step, etc.\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits, name='cross_ent_terms'), name='cross_ent')\n",
        "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss)\n",
        "    \n",
        "    def sample(self, x): \n",
        "        # evaluate network and draw from resulting softmax multinomials.\n",
        "        pred_vals = self.session.run(self.ypred, feed_dict={self.x:np.reshape(x,[-1,self.n_in])})\n",
        "        return [np.random.multinomial(1,pvals=pv) for pv in pred_vals]\n",
        "\n",
        "    def compute_logits(self, x):\n",
        "        # evaluate the network and return the logit values\n",
        "        return self.session.run(self.logits, feed_dict={self.x:np.reshape(x,[-1,self.n_in])})\n",
        "    \n",
        "    def train(self, x_batch, y_batch):\n",
        "        # take a training step\n",
        "        _ = self.session.run(self.train_step, feed_dict={self.x: x_batch, self.y: y_batch})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwwLcoUtEEne"
      },
      "source": [
        "### Run simple example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBdi9gVNEEnf"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    with tf.Session() as sess:\n",
        "    #with tf.compat.v1.Session() as sess:\n",
        "        # create a Network\n",
        "        g = Network0(sess, n_in, n_in)\n",
        "        # usual tf initialization\n",
        "        sess.run(tf.global_variables_initializer())      \n",
        "        ####\n",
        "        # write some text before any training\n",
        "        ####\n",
        "        x_prev = np.floor(np.random.rand(1)*(n_in)).astype(int)\n",
        "        text0 = ind2ch[x_prev[0]]\n",
        "        for i in range(90):\n",
        "            x = np.argmax(g.compute_logits(ind2onehot(x_prev)))\n",
        "            #x = np.argmax(g.sample(ind2onehot(x_prev)))\n",
        "            text0 += ind2ch[x]\n",
        "            x_prev = x\n",
        "        \n",
        "        ####\n",
        "        # Train on some digits\n",
        "        ####\n",
        "        for i in range(5001):\n",
        "            batch = np.floor(np.random.rand(batch_size)*(n_train-1)).astype(int)\n",
        "            x_batch = ch2onehot([all_text[b] for b in batch])\n",
        "            y_batch = ch2onehot([all_text[b+1] for b in batch])\n",
        "\n",
        "            # now run\n",
        "            g.train(x_batch,y_batch)\n",
        "        \n",
        "        \n",
        "        ####\n",
        "        # write some text after some training\n",
        "        ####\n",
        "        x_prev = np.floor(np.random.rand(1)*(n_in)).astype(int)\n",
        "        text1 = ind2ch[x_prev[0]]\n",
        "        for i in range(90):\n",
        "            x = np.argmax(g.compute_logits(ind2onehot(x_prev)))\n",
        "            #x = np.argmax(g.sample(ind2onehot(x_prev)))\n",
        "            text1 += ind2ch[x]\n",
        "            x_prev = x\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx_4JHMwEEng"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdCsXeOmEEng"
      },
      "outputs": [],
      "source": [
        "print('----Pre-training Sample----\\n {}'.format(text0))\n",
        "#print('\\n')\n",
        "print('----Post-training Sample----\\n {}'.format(text1))\n",
        "# why is this the best we can do? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1MWkvlqEEng"
      },
      "source": [
        "### Now a Network with a hidden state (but no tf RNN abstractions yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU-PB9iVEEnh"
      },
      "outputs": [],
      "source": [
        "class Network1:\n",
        "    \n",
        "    def __init__(self, session, n_in , n_out, n_context, n_hidden=64, rnn_type='1layer'):\n",
        "        self.session = session\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.n_context = n_context\n",
        "        self.n_hidden = n_hidden\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        # data placeholders\n",
        "        self.x = tf.placeholder(tf.float32, [None, self.n_context, self.n_in], name='x')\n",
        "        self.y = tf.placeholder(tf.float32, [None, self.n_context, self.n_out], name='y')\n",
        "        self.x_step = tf.placeholder(tf.float32, [1, self.n_in], name='x_step')\n",
        "        #self.h_ = tf.placeholder(tf.float32,[None, self.n_hidden], name='h_')\n",
        "        # initial hidden state\n",
        "        self.h_ = tf.zeros([1,self.n_hidden])  # notice this 1 is inducing some broadcasting\n",
        "        \n",
        "        # define RNN\n",
        "        self.Wyh = tf.get_variable('Why', shape=[self.n_hidden,self.n_out])\n",
        "        self.by = tf.get_variable('by', shape=[self.n_out])\n",
        "        if self.rnn_type=='1layer':\n",
        "            self.Wxh = tf.get_variable('Wxh', shape=[self.n_in,self.n_hidden])\n",
        "            self.bh = tf.get_variable('bh', shape=[self.n_hidden])\n",
        "            self.Whh = tf.get_variable('Whh', shape=[self.n_hidden,self.n_hidden])        \n",
        "        elif self.rnn_type=='2layer':\n",
        "            self.n2 = 256\n",
        "            self.Wx1 = tf.get_variable('Wx1', shape=[self.n_in,self.n2])\n",
        "            self.b1 = tf.get_variable('b1', shape=[self.n2])\n",
        "            self.Wh1 = tf.get_variable('Wh1', shape=[self.n_hidden,self.n2])        \n",
        "            self.W1h = tf.get_variable('W1h', shape=[self.n2,self.n_hidden])\n",
        "            self.bh = tf.get_variable('bh', shape=[self.n_hidden])\n",
        "            \n",
        "        # split (and squeeze) to get BPTT inputs, that is, a list of length n_context with usual [batch_size,n_in]\n",
        "        # note: see code at bottom of notebook for critical \",[1]\" fix\n",
        "        self.xs = [tf.squeeze(xx,[1]) for xx in tf.split(self.x, self.n_context, axis=1)] \n",
        "        self.ys = [tf.squeeze(yy,[1]) for yy in tf.split(self.y, self.n_context, axis=1)] \n",
        "        \n",
        "        # propagate h through context length\n",
        "        self.h = []\n",
        "        h = self.h_\n",
        "        for x in self.xs:\n",
        "            # here the first time h_ is broadcast to the np.shape(x,0) (as in, batch_size)\n",
        "            h = self.rnn_layer(x,h) #tf.nn.tanh(tf.matmul(x, self.Wxh) + tf.matmul(h, self.Whh) + self.bh)\n",
        "            self.h.append(h)\n",
        "        \n",
        "        # make outputs from h\n",
        "        with tf.name_scope('model'):\n",
        "            self.logits = []\n",
        "            self.ypred = []\n",
        "            for h in self.h:\n",
        "                logits = self.rnn_logit(h)\n",
        "                self.logits.append(logits)\n",
        "                self.ypred.append(tf.nn.softmax(logits))\n",
        "        \n",
        "            # conform sizes with expectation\n",
        "            self.ypred = tf.transpose(self.ypred,[1,0,2])\n",
        "\n",
        "        # loss, train_step, etc.\n",
        "        with tf.name_scope('loss'):\n",
        "            self.losses = []\n",
        "            self.accuracies = []\n",
        "            for (l,y) in zip(self.logits,self.ys):\n",
        "                self.losses.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=l)))\n",
        "                self.accuracies.append(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(l,1), tf.argmax(y,1)), tf.float32)))\n",
        "            self.loss = tf.reduce_mean(self.losses)\n",
        "            self.accuracy = tf.reduce_mean(self.accuracies)\n",
        "                                       \n",
        "        with tf.name_scope('opt'):\n",
        "            self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss)\n",
        "    \n",
        "        # ops to propagate the network forward a step (for example, sampling after learned parameter)\n",
        "        with tf.name_scope('step'):\n",
        "            self.h_next = self.rnn_layer(self.x_step, self.h_)\n",
        "            self.logit_next = self.rnn_logit(self.h_next)\n",
        "    \n",
        "        with tf.name_scope('summaries'):\n",
        "            # create summary for loss and accuracy\n",
        "            tf.summary.scalar('loss', self.loss) \n",
        "            tf.summary.scalar('accuracy', self.accuracy)\n",
        "            # create summary for logits\n",
        "            tf.summary.histogram('logits', self.logits)\n",
        "            # create summary for input image\n",
        "            #tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "        \n",
        "    def rnn_layer(self,x,h):\n",
        "        with tf.name_scope('rnn_layer'):\n",
        "            # this can be called either via training or stepping \n",
        "            if self.rnn_type=='1layer':\n",
        "                return tf.nn.tanh(tf.matmul(x, self.Wxh) + tf.matmul(h, self.Whh) + self.bh)\n",
        "            elif self.rnn_type=='2layer':\n",
        "                fc1 = tf.nn.relu(tf.matmul(x, self.Wx1) + tf.matmul(h, self.Wh1) + self.b1)\n",
        "                return tf.nn.tanh(tf.matmul(fc1, self.W1h) + self.bh)\n",
        "            \n",
        "    def rnn_logit(self,h):\n",
        "        # called either via training or stepping\n",
        "        with tf.name_scope('rnn_logit'):\n",
        "            return tf.matmul(h, self.Wyh) + self.by\n",
        "    \n",
        "    def train(self, x_batch, y_batch, h_):\n",
        "        # take a training step.\n",
        "        _, h_out, loss, summary = self.session.run((self.train_step, self.h, self.loss, self.summary_op), feed_dict=\n",
        "                                          {self.x: np.reshape(ch2onehot(x_batch),[-1, self.n_context, self.n_in]), \n",
        "                                           self.y: np.reshape(ch2onehot(y_batch),[-1, self.n_context, self.n_in]), \n",
        "                                           self.h_: h_})\n",
        "        #self.session.run(self.train_step, feed_dict={self.x: np.reshape(ch2onehot(x_batch),[-1, self.n_context, self.n_in]), self.y: np.reshape(ch2onehot(y_batch),[-1, self.n_context, self.n_in])})\n",
        "        # return the last hidden state h, which will seed the next batch.\n",
        "        return (h_out[-1], loss, summary)\n",
        "    \n",
        "    def sample_step(self, x_step, h_ , sample=False, temp=1.0):\n",
        "        # take a forward step, predict the next character, return the new hidden state.\n",
        "        h_next, logit_next = self.session.run((self.h_next, self.logit_next), feed_dict=\n",
        "                                          {self.x_step: np.reshape(ch2onehot(x_step),[1, self.n_in]), \n",
        "                                           self.h_: h_})\n",
        "        #self.session.run(self.train_step, feed_dict={self.x: np.reshape(ch2onehot(x_batch),[-1, self.n_context, self.n_in]), self.y: np.reshape(ch2onehot(y_batch),[-1, self.n_context, self.n_in])})\n",
        "        # return the last hidden state h, which will seed the next batch.\n",
        "        if sample: \n",
        "            # sample from multinomial\n",
        "            y_out = ind2ch[np.argmax(np.random.multinomial(1,pvals=softmax([temp*l for l in logit_next[0]]) ))]\n",
        "        else:\n",
        "            y_out = ind2ch[np.argmax(logit_next)]\n",
        "            \n",
        "        return (h_next, y_out)\n",
        "    \n",
        "    def sample_text(self, seed_char , m , sample=False):\n",
        "        h = np.zeros([1,rnn.n_hidden])\n",
        "        text_out = seed_char\n",
        "        for j in range(100):\n",
        "            # roll forward and predict text\n",
        "            h, y = self.sample_step(text_out[-1],h)\n",
        "            text_out += y\n",
        "        return text_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHWBPDcjEEni"
      },
      "source": [
        "### Run it..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXroM2atEEni"
      },
      "outputs": [],
      "source": [
        "n_context = 50\n",
        "model_type = '1layer'\n",
        "n_hidden = 64\n",
        "dir_name = 'logs/scratch07/{}_{}_{}'.format(model_type, n_hidden, n_context)\n",
        "#all_text = 'The quick brown fox jumped over the lazy dog. '*1000\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    with tf.Session() as sess:\n",
        "        # create a Network\n",
        "        rnn = Network1(sess, n_in, n_in, n_context, n_hidden, model_type)\n",
        "        # make summarywriter for tb\n",
        "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
        "        # usual tf initialization\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        \n",
        "        # training \n",
        "        # walk through data from start to finish.  Walk through in blocks of BPTT\n",
        "        epoch = 0\n",
        "        batch = 0\n",
        "        batches_per_epoch = np.floor(len(all_text)/n_context)\n",
        "        h_prev = np.zeros([1,rnn.n_hidden])\n",
        "        #losses =[]\n",
        "        while epoch < 7:\n",
        "            if (batch+1)*n_context+1  > (len(all_text)-1):\n",
        "                # wrap to beginning and reset\n",
        "                batch = 0\n",
        "                epoch += 1\n",
        "                h_prev = np.zeros([1,rnn.n_hidden])\n",
        "            # assign data\n",
        "            x_batch = all_text[batch*n_context:(batch+1)*n_context]\n",
        "            y_batch = all_text[batch*n_context+1:(batch+1)*n_context+1]\n",
        "            \n",
        "            # training step\n",
        "            h_prev, loss, summary = rnn.train( x_batch, y_batch, h_prev )\n",
        "            \n",
        "            # iterate\n",
        "            batch += 1\n",
        "            #losses.append(loss)\n",
        "            # print diagnostic\n",
        "            if batch%1000==0:\n",
        "                k = (epoch*batches_per_epoch + batch).astype(int)\n",
        "                summary_writer.add_summary(summary, k)\n",
        "                print('______[epoch:{},batch:{},all batches:{}] has loss {}______'.format(epoch,batch,k,loss))\n",
        "                # take the last hidden and target to seed a writing\n",
        "                h = h_prev \n",
        "                text_out = y_batch[-1]\n",
        "                for j in range(200):\n",
        "                    # roll forward and fantasize text of length 200\n",
        "                    h, y = rnn.sample_step(text_out[-1],h, sample=True, temp=min(batch/5000,5))\n",
        "                    text_out += y\n",
        "                print(text_out)\n",
        "                print('')\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXnULLGmEEnj"
      },
      "source": [
        "### Now an LSTM network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXZkMm2rEEnj"
      },
      "outputs": [],
      "source": [
        "class Network2:\n",
        "    \n",
        "    def __init__(self, session, n_in , n_out, n_context, n_hidden=64, rnn_type='1layer'):\n",
        "        self.session = session\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.n_context = n_context\n",
        "        self.n_hidden = n_hidden\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        # data placeholders\n",
        "        self.x = tf.placeholder(tf.float32, [None, self.n_context, self.n_in], name='x')\n",
        "        self.y = tf.placeholder(tf.float32, [None, self.n_context, self.n_out], name='y')\n",
        "        self.batch_size = tf.shape(self.x)[0]  # 0 for time_major=False in dynamic_rnn; else 1 for True\n",
        "\n",
        "        self.x_step = tf.placeholder(tf.float32, [None, 1, self.n_in], name='x_step')\n",
        "        # initial hidden state; None here is batch size... context is not needed as it is t=0\n",
        "        self.c_ = tf.placeholder(tf.float32, [None,self.n_hidden], name='c_')\n",
        "        self.h_ = tf.placeholder(tf.float32, [None,self.n_hidden], name='h_')\n",
        "        # An LSTMStateTuple that can be fed as initial_state to dynamic_rnn\n",
        "        self.state_ = tf.nn.rnn_cell.LSTMStateTuple(self.c_, self.h_)  # 2 x None x n_hidden\n",
        "        \n",
        "        # define RNN\n",
        "        self.Wyh = tf.get_variable('Why', shape=[self.n_hidden,self.n_out])\n",
        "        self.by = tf.get_variable('by', shape=[self.n_out])\n",
        "        self.cell = tf.contrib.rnn.LSTMCell(self.n_hidden)\n",
        "        # If cells are LSTMCells state will be a tuple containing a LSTMStateTuple for each cell.\n",
        "        h_outs, self.state_out = tf.nn.dynamic_rnn(self.cell, self.x, initial_state=self.state_)\n",
        "        # time_major=True implies time, batch, depth; see https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
        "        # time_major=False implies batch, time, depth\n",
        "\n",
        "        # now h_outs is batch,time, hidden size\n",
        "        self.h = tf.reshape(h_outs,[-1,self.n_hidden])\n",
        "        with tf.name_scope('model'):\n",
        "            self.logits = self.rnn_logit(self.h)\n",
        "            self.ypred = tf.nn.softmax(self.logits)\n",
        "            \n",
        "        # loss, train_step, etc.\n",
        "        with tf.name_scope('loss'):\n",
        "            y = tf.reshape(self.y,[-1,self.n_out]) # conform this to the unfolded matrix shape\n",
        "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=self.logits))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.logits,1), tf.argmax(y,1)),tf.float32))\n",
        "                                       \n",
        "        with tf.name_scope('opt'):\n",
        "            self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss)\n",
        "    \n",
        "        # ops to propagate the network forward a step (for example, sampling after learned parameter)\n",
        "        with tf.name_scope('step'):\n",
        "            #self.h_next = self.rnn_layer(self.x_step, self.state_)\n",
        "            self.h_step, self.state_step = tf.nn.dynamic_rnn(self.cell, self.x_step, initial_state=self.state_)\n",
        "            self.logit_step = self.rnn_logit(self.h_step[0])\n",
        "    \n",
        "        with tf.name_scope('summaries'):\n",
        "            # create summary for loss and accuracy\n",
        "            tf.summary.scalar('loss', self.loss) \n",
        "            tf.summary.scalar('accuracy', self.accuracy)\n",
        "            # create summary for logits\n",
        "            tf.summary.histogram('logits', self.logits)\n",
        "            # create summary for input image\n",
        "            #tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "        \n",
        "    def rnn_logit(self,h):\n",
        "        # called either via training or stepping\n",
        "        with tf.name_scope('rnn_logit'):\n",
        "            return tf.matmul(h, self.Wyh) + self.by\n",
        "    \n",
        "    def train(self, x_batch, y_batch, c_, h_):\n",
        "        # take a training step.\n",
        "        # note this is clunky... caller must partition state_out[0] as c_, state_out[1] as h_ when iterating\n",
        "        _, state_out, loss, summary = self.session.run((self.train_step, self.state_out, self.loss, self.summary_op), feed_dict=\n",
        "                                          {self.x: np.reshape(ch2onehot(x_batch),[-1, self.n_context, self.n_in]), \n",
        "                                           self.y: np.reshape(ch2onehot(y_batch),[-1, self.n_context, self.n_in]), \n",
        "                                           self.c_: c_,\n",
        "                                           self.h_: h_})\n",
        "        #self.session.run(self.train_step, feed_dict={self.x: np.reshape(ch2onehot(x_batch),[-1, self.n_context, self.n_in]), self.y: np.reshape(ch2onehot(y_batch),[-1, self.n_context, self.n_in])})\n",
        "        # return the last hidden state h, which will seed the next batch.\n",
        "        return (state_out, loss, summary)\n",
        "    \n",
        "    def sample_step(self, x_step, c_, h_ , sample=False, temp=1.0):\n",
        "        # take a forward step, predict the next character, return the new hidden state.\n",
        "        # note this is clunky... caller must partition state_out[0] as c_, state_out[1] as h_ when iterating\n",
        "        state_step, logit_step = self.session.run((self.state_step, self.logit_step), feed_dict=\n",
        "                                          {self.x_step: np.reshape(ch2onehot(x_step),[1, 1, self.n_in]), \n",
        "                                           self.c_: c_,\n",
        "                                           self.h_: h_})\n",
        "        #self.session.run(self.train_step, feed_dict={self.x: np.reshape(ch2onehot(x_batch),[-1, self.n_context, self.n_in]), self.y: np.reshape(ch2onehot(y_batch),[-1, self.n_context, self.n_in])})\n",
        "        # return the last hidden state h, which will seed the next batch.\n",
        "        if sample: \n",
        "            # sample from multinomial\n",
        "            y_out = ind2ch[np.argmax(np.random.multinomial(1,pvals=softmax([temp*l for l in logit_step[0]]) ))]\n",
        "        else:\n",
        "            y_out = ind2ch[np.argmax(logit_step)]\n",
        "            \n",
        "        return (state_step, y_out)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpH5nSs7EEnk"
      },
      "source": [
        "### Run LSTM..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkVChWCnEEnk"
      },
      "outputs": [],
      "source": [
        "n_context = 50\n",
        "model_type = 'lstm'\n",
        "n_hidden = 256\n",
        "dir_name = 'logs/scratch07/{}_{}_{}'.format(model_type, n_hidden, n_context)\n",
        "#all_text = 'The quick brown fox jumped over the lazy dog. '*1000\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    with tf.Session() as sess:\n",
        "        # create a Network\n",
        "        rnn = Network2(sess, n_in, n_in, n_context, n_hidden, model_type)\n",
        "        # make summarywriter for tb\n",
        "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
        "        # usual tf initialization\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        \n",
        "        # training \n",
        "        # walk through data from start to finish.  Walk through in blocks of BPTT\n",
        "        epoch = 0\n",
        "        batch = 0\n",
        "        batches_per_epoch = np.floor(len(all_text)/n_context)\n",
        "        \n",
        "        state_prev = np.zeros([2,1,rnn.n_hidden])\n",
        "        state_prev[0] = np.zeros([1,rnn.n_hidden])\n",
        "        state_prev[1] = np.zeros([1,rnn.n_hidden])\n",
        "        #losses =[]\n",
        "        while epoch < 15:\n",
        "            if (batch+1)*n_context+1+epoch  > (len(all_text)-1):\n",
        "                # wrap to beginning and reset\n",
        "                batch = 0\n",
        "                epoch += 1\n",
        "                h_prev = np.zeros([1,rnn.n_hidden])\n",
        "            # assign data, shifting by 1 each epoch \n",
        "            x_batch = all_text[batch*n_context+epoch:(batch+1)*n_context+epoch]\n",
        "            y_batch = all_text[batch*n_context+1+epoch:(batch+1)*n_context+1+epoch]\n",
        "            \n",
        "            # training step\n",
        "            state_prev, loss, summary = rnn.train( x_batch, y_batch, state_prev[0], state_prev[1] )\n",
        "            \n",
        "            # iterate\n",
        "            batch += 1\n",
        "            #losses.append(loss)\n",
        "            # print diagnostic\n",
        "            if batch%1000==0:\n",
        "                k = (epoch*batches_per_epoch + batch).astype(int)\n",
        "                summary_writer.add_summary(summary, k)\n",
        "                print('______[epoch:{},batch:{},all batches:{}] has loss {}______'.format(epoch,batch,k,loss))\n",
        "                # take the last hidden and target to seed a writing\n",
        "                h = state_prev \n",
        "                text_out = y_batch[-1]\n",
        "                for j in range(200):\n",
        "                    # roll forward and predict text\n",
        "                    h, y = rnn.sample_step(text_out[-1], h[0] , h[1] , sample=True, temp=min(batch/5000,5))\n",
        "                    text_out += y\n",
        "                print(text_out)\n",
        "                print('')\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Ba3U9-EEnl"
      },
      "source": [
        "### Slide extras; no didactic purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qzv9GEgEEnl"
      },
      "outputs": [],
      "source": [
        "# Test code to play with LSTMStateTuple object...\n",
        "x_in = np.random.rand(5,30,4)\n",
        "h0 = np.zeros([5,22])\n",
        "# time_major=True implies time, batch, input size; see https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
        "# time_major=False implies batch, time, input size\n",
        "with tf.Graph().as_default():\n",
        "    with tf.Session() as sess:\n",
        "        # create a Network\n",
        "        x = tf.placeholder(tf.float32, [None,None,4], name='x')\n",
        "        c_state = tf.placeholder(tf.float32, [None,22], name='c')\n",
        "        h_state = tf.placeholder(tf.float32, [None,22], name='h')\n",
        "        initial_state = tf.nn.rnn_cell.LSTMStateTuple(c_state, h_state)\n",
        "\n",
        "        # here's the RNN\n",
        "        cell = tf.contrib.rnn.LSTMCell(22)\n",
        "        \n",
        "        batch_size    = tf.shape(x)[0]  # 0 for time_major=False in dynamic_rnn; else 1 for True\n",
        "        h_ = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "        #lstmx,lstmh = cell(x, h)\n",
        "        rnn_outputs, rnn_state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32, initial_state=initial_state)\n",
        "\n",
        "        # usual tf initialization\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        out = sess.run(rnn_state, feed_dict={x:x_in, c_state:h0, h_state:h0})\n",
        "print(np.shape(out))\n",
        "#out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oChuS2HgEEnm"
      },
      "outputs": [],
      "source": [
        "# note, passing in batches of size 1 can trigger an issue because the squeeze below will dump dimension 0\n",
        "# this is then a problem because matmul gets a [,4] instead of a matrix [1,4] (that it wants).\n",
        "# you can sort of fix this for 1,2,4 (1 here is the batch size) by changing to tf.matmul([xx],W)\n",
        "# but then of course that's a problem for 11,2,4 (or whatever batch size) because then a nested matrix\n",
        "# the right solve is to squeeze carefully... as it is now\n",
        "x_in = np.random.rand(2,2,4)\n",
        "with tf.Graph().as_default():\n",
        "    with tf.Session() as sess:\n",
        "        # create a Network\n",
        "        x = tf.placeholder(tf.float32, [None, 2, 4], name='x')\n",
        "        W = tf.get_variable('W', shape=[4,3])\n",
        "        def f(x):\n",
        "            return tf.matmul(x, W)\n",
        "        x_step = tf.placeholder(tf.float32, [2, 4], name='x')\n",
        "        def g(x):\n",
        "            return tf.matmul(x, W)\n",
        "        \n",
        "        xs = [tf.squeeze(xx,[1]) for xx in tf.split(x, 2, axis=1)] \n",
        "        xW = []\n",
        "        for xx in xs:\n",
        "            xW.append(f(xx))\n",
        "        \n",
        "        xW_step = g(x_step)\n",
        "        \n",
        "        # usual tf initialization\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        out = sess.run(xW, feed_dict={x:x_in})\n",
        "        out_step = sess.run(xW_step, feed_dict={x_step:x_in[0,:,:]})\n",
        "        \n",
        "print(out)\n",
        "print(out_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nxcNOhXEEnm"
      },
      "outputs": [],
      "source": [
        "def compute_logits(self, text_in):\n",
        "        # evaluate the network and return the logit values\n",
        "        return self.session.run(self.logits, feed_dict={self.x:np.reshape(ch2onehot(text_in),[-1, self.n_context, self.n_in])})\n",
        "    \n",
        "    def sample_text(self, text_in): \n",
        "        # evaluate network and draw from resulting softmax multinomials.\n",
        "        pred_vals = self.session.run(self.ypred, feed_dict={self.x:np.reshape(ch2onehot(text_in),[-1, self.n_context, self.n_in])})\n",
        "        #return pred_vals \n",
        "        return [ind2ch[np.argmax(np.random.multinomial(1,pvals=pv))] for pv in pred_vals.reshape([-1,n_in])]\n",
        "        #[np.random.multinomial(1,pvals=pv) for pv in pred_vals]\n",
        "    \n",
        "    def predict_text(self, text_in): \n",
        "        # evaluate network and draw from resulting softmax multinomials.\n",
        "        pred_vals = self.session.run(self.ypred, feed_dict={self.x:np.reshape(ch2onehot(text_in),[-1, self.n_context, self.n_in])})\n",
        "        return [ind2ch[np.argmax(pv)] for pv in pred_vals.reshape([-1,n_in])]\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN8Fq841EEnn"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses, linewidth=2)\n",
        "plt.xlabel('every 1000 contexts')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Simple RNN with hidden state')\n",
        "plt.show()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCinKJE9EEnn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7DGaOpEEnn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd-WRupsEEnn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21uia4mWEEno"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UoVrD_IEEno"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ03iiWwEEnp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GSb0w-HEEnp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeJsk1vrEEnp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA4Yz45HEEnp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdLN5UrvEEnq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "meet_shakespeare_with_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}